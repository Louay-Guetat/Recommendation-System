# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QTwwRswV2_DCA_jdvnG4z0DWyjAFyXwB
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import pandas as pd
import os
import warnings
import networkx as nx
from node2vec import Node2Vec
from sklearn.manifold import TSNE
import gensim
from gensim.models import CoherenceModel
from gensim.utils import simple_preprocess
from gensim import corpora
from gensim.models import LdaModel
from gensim.corpora import Dictionary
from gensim.models.coherencemodel import CoherenceModel
from gensim.utils import simple_preprocess

top_k = 5

warnings.filterwarnings("ignore")
np.random.seed(2)

df = pd.read_csv("/home/louay/Desktop/PI/deployment/RecommenderSystem/mainApp/models/df_cleaned.csv")

df.columns

def get_optimal_num_topics(text_data):
    tokenized_text = [simple_preprocess(text) for text in text_data]
    dictionary = corpora.Dictionary(tokenized_text)
    corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_text]

    coherence_values = []
    model_list = []

    for num_topics in range(8, 15):  # Try a range of topic numbers
        lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=100)
        model_list.append(lda_model)
        coherence_model = CoherenceModel(model=lda_model, texts=tokenized_text, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherence_model.get_coherence())

    # Select the number of topics with the highest coherence value
    optimal_num_topics = range(8, 15)[coherence_values.index(max(coherence_values))]
    return optimal_num_topics

# Determine the optimal number of topics
optimal_topics = get_optimal_num_topics(df['Text'].tolist())

print(f"Optimal number of topics: {optimal_topics}")
from gensim.models import LdaModel
from gensim.corpora import Dictionary

def train_lda_with_optimal_topics(text_data, num_topics):
    tokenized_text = [simple_preprocess(text) for text in text_data]
    dictionary = Dictionary(tokenized_text)
    corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_text]

    # Train the LDA model with the specified number of topics
    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=100)

    return lda_model, corpus

# Train the LDA model with 6 topics
optimal_lda_model, optimal_corpus = train_lda_with_optimal_topics(df['Text'].tolist(), optimal_topics)

# Function to get the dominant topic for a document
def get_dominant_topic(lda_model, doc):
    topics = lda_model.get_document_topics(doc)
    dominant_topic = max(topics, key=lambda x: x[1])
    return dominant_topic[0]  # Returning the topic ID

# Add the topic as a new column in the DataFrame
df['Topic'] = [get_dominant_topic(optimal_lda_model, doc) for doc in optimal_corpus]
topic_counts = df['Topic'].value_counts()
topic_counts

import networkx as nx
from node2vec import Node2Vec
from torch_geometric.data import Data
import torch
from transformers import BertTokenizer, BertModel
import pandas as pd
from fuzzywuzzy import process

# Assuming df is your DataFrame
# Create a graph object
G = nx.Graph()

# Include additional node attributes
for _, row in df.iterrows():
    title = row['Concepts']
    genre = row['Object']
    relation = row['Predicate']

    G.add_node(title)
    G.add_node(genre)
    G.add_edge(title, genre, relation=relation)

# Use BERT for text embeddings with updated embedding size to match Node2Vec (64 dimensions)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

text_embeddings = {}
for column in ['Refrences', 'def', 'Keywords', 'Synonyms']:
    for node in G.nodes():
        matching_rows = df[df['Concepts'] == node]
        if not matching_rows.empty:
            text = matching_rows[column].values[0]
            inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True)
            outputs = model(**inputs)

            # Update the BERT embedding size to match Node2Vec (64 dimensions)
            embedding = outputs['last_hidden_state'][:, 0, :].squeeze().detach().numpy()  # Change the dimension to (64,)

            if node not in text_embeddings:
                text_embeddings[node] = {}
            text_embeddings[node][column] = embedding.tolist()

# Use Node2Vec for node embeddings
node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)
node2vec_model = node2vec.fit(window=10, min_count=1, batch_words=4)
node_embeddings = {node: node2vec_model.wv[node] for node in G.nodes}

# Combine node embeddings with default embedding for missing nodes
default_embedding = torch.zeros(64)  # Change the dimension to match Node2Vec embeddings

combined_embeddings = {}
for node in G.nodes:
    # Check if the node is present in either 'Object' or 'Concepts' column
    if node in df['Concepts'].values:
        combined_embedding = torch.cat([
            torch.tensor(node_embeddings.get(node, default_embedding)),
            torch.tensor(text_embeddings[node]['Refrences']),
            torch.tensor(text_embeddings[node]['def']),
            torch.tensor(text_embeddings[node]['Keywords']),
            torch.tensor(text_embeddings[node]['Synonyms'])
        ])
        combined_embeddings[node] = combined_embedding
    else:
        #print(f"Node '{node}' not found in either 'Object' or 'Concepts' column.")
        continue

# Map node names to numerical indices
node_indices = {node: idx for idx, node in enumerate(G.nodes)}

# Now create the data object for torch_geometric
embedding_matrix = torch.stack(list(combined_embeddings.values()))
edges = torch.tensor([(node_indices[u], node_indices[v]) for u, v in G.edges]).t().contiguous()
edge_index = torch.cat((edges, edges.flip(0)), dim=1)
data = Data(x=embedding_matrix, edge_index=edge_index)
# Assuming df has a 'Topic' column with node labels
node_labels = df['Topic'].astype('category').cat.codes.values

# Add 'y' key to the data object
data.y = torch.tensor(node_labels, dtype=torch.long)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool
from torch_geometric.data import Data
from sklearn.model_selection import train_test_split
import pandas as pd
from torch.optim.lr_scheduler import StepLR
from torch_geometric.nn import GATConv
import random


# Set random seeds for reproducibility
seed = 42
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
np.random.seed(seed)
random.seed(seed)

# Define the GNN model with dropout
class GNNModel(nn.Module):

    def __init__(self, input_dim, embedding_dim=64, num_heads=4, weight_decay=1e-5):
        super(GNNModel, self).__init__()
        self.conv1 = GATConv(input_dim, embedding_dim, heads=num_heads, dropout=0.25)
        self.conv2 = GATConv(embedding_dim * num_heads, embedding_dim, heads=num_heads, dropout=0.25)

        # Calculate the output size after GNN layers
        gnn_output_size = embedding_dim * num_heads * 2  # Adjust if necessary

        # Modify the linear layer to match the output size after GNN
        self.fc = nn.Linear(gnn_output_size, 1)

        # Add L2 regularization to GNN layers
        self.conv1.weight_decay = weight_decay
        self.conv2.weight_decay = weight_decay

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))

        x_mean = global_mean_pool(x, data.batch)
        x_max = global_max_pool(x, data.batch)
        x = torch.cat([x_mean, x_max], dim=1)

        x = self.fc(x)
        return x
    # def __init__(self, input_dim, embedding_dim=64, weight_decay=1e-5):
    #     super(GNNModel, self).__init__()
    #     self.conv1 = GATConv(input_dim, embedding_dim)
    #     self.dropout1 = nn.Dropout(p=0.25)
    #     self.conv2 = GCNConv(embedding_dim, embedding_dim)
    #     self.dropout2 = nn.Dropout(p=0.2)
    #     self.fc1 = nn.Linear(embedding_dim * 2, 64)  # Additional hidden layer
    #     self.fc2 = nn.Linear(64, 1)  # Output layer

    #     # Add L2 regularization to GNN layers
    #     self.conv1.weight_decay = weight_decay
    #     self.conv2.weight_decay = weight_decay
    #     self.fc1.weight_decay = weight_decay
    #     self.fc2.weight_decay = weight_decay

    # def forward(self, data):
    #     x, edge_index = data.x, data.edge_index

    #     x = self.conv1(x, edge_index)
    #     x = F.elu(x)
    #     x = self.dropout1(x)
    #     x = self.conv2(x, edge_index)
    #     x = F.relu(x)
    #     x = self.dropout2(x)

    #     x_mean = global_mean_pool(x, data.batch)
    #     x_max = global_max_pool(x, data.batch)
    #     x = torch.cat([x_mean, x_max], dim=1)

    #     x = F.relu(self.fc1(x))  # Apply ReLU activation to the additional hidden layer
    #     x = self.fc2(x)  # Output layer
    #     return x

# Convert data to a DataFrame
df_data = pd.DataFrame(data.x.numpy(), columns=[f'feat_{i}' for i in range(data.x.shape[1])])
df_data['y'] = df['Topic'][:data.num_nodes].values  # Use only the labels for the existing nodes
df_data.reset_index(drop=True, inplace=True)

# Split data into train, validation, and test sets
train_data, temp_data = train_test_split(df_data, test_size=0.3, random_state=seed)
validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=seed)

# Get the nodes in the training, validation, and test sets
train_nodes = set(train_data.index)
validation_nodes = set(validation_data.index)
test_nodes = set(test_data.index)

# Filter edge indices based on nodes in the training set
train_edge_index = data.edge_index[:, [i for i in range(data.edge_index.shape[1]) if data.edge_index[0, i] in train_nodes or data.edge_index[1, i] in train_nodes]]

# Filter edge indices based on nodes in the validation set
validation_edge_index = data.edge_index[:, [i for i in range(data.edge_index.shape[1]) if data.edge_index[0, i] in validation_nodes or data.edge_index[1, i] in validation_nodes]]

# Filter edge indices based on nodes in the test set
test_edge_index = data.edge_index[:, [i for i in range(data.edge_index.shape[1]) if data.edge_index[0, i] in test_nodes or data.edge_index[1, i] in test_nodes]]

# Convert train, validation, and test sets back to Data objects
train_data = Data(x=torch.tensor(train_data.iloc[:, :-1].values, dtype=torch.float32),
                  edge_index=train_edge_index,  # Use filtered edge_index for training
                  y=torch.tensor(train_data['y'].values, dtype=torch.float32))
validation_data = Data(x=torch.tensor(validation_data.iloc[:, :-1].values, dtype=torch.float32),
                       edge_index=validation_edge_index,  # Use filtered edge_index for validation
                       y=torch.tensor(validation_data['y'].values, dtype=torch.float32))
test_data = Data(x=torch.tensor(test_data.iloc[:, :-1].values, dtype=torch.float32),
                 edge_index=test_edge_index,  # Use filtered edge_index for testing
                 y=torch.tensor(test_data['y'].values, dtype=torch.float32))

# Initialize the GNN model
model = GNNModel(input_dim=embedding_matrix.size(1), embedding_dim=64, num_heads=4)#, dropout=0.5)
# Update input_dim to match the size of the node embeddings

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)

# Define a learning rate scheduler
scheduler = StepLR(optimizer, step_size=20, gamma=0.5)

# Early stopping parameters
best_val_loss = float('inf')
patience = 5  # Adjusted patience
patience_counter = 1

# Validation interval
validation_interval = 5

# Training loop
num_epochs = 200
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(train_data)
    loss = criterion(outputs, train_data.y)
    loss.backward()
    optimizer.step()
    scheduler.step()

    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')

    # Validation and early stopping
    if epoch % validation_interval == 0:
        model.eval()
        with torch.no_grad():
            val_outputs = model(validation_data)
            val_loss = criterion(val_outputs, validation_data.y)

        print(f'Validation Loss: {val_loss.item()}')

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0  # Reset patience counter
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f'Early stopping at epoch {epoch}')
                break

# Evaluation
model.eval()
with torch.no_grad():
    test_outputs = model(test_data).view(-1)  # Reshape to match the shape of test_data.y
    mse = F.mse_loss(test_outputs, test_data.y)
    rmse = torch.sqrt(mse)
    print(f'Mean Squared Error on Test Set: {rmse}')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import pandas as pd
import os
import warnings
import networkx as nx
from node2vec import Node2Vec
from sklearn.manifold import TSNE
import gensim
from gensim.models import CoherenceModel
from gensim.utils import simple_preprocess
from gensim import corpora
from gensim.models import LdaModel
from gensim.corpora import Dictionary

warnings.filterwarnings("ignore")
np.random.seed(2)

df = pd.read_csv("/home/louay/Desktop/PI/deployment/RecommenderSystem/mainApp/models/df_cleaned.csv")

df.columns

def get_optimal_num_topics(text_data):
    tokenized_text = [simple_preprocess(text) for text in text_data]
    dictionary = corpora.Dictionary(tokenized_text)
    corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_text]

    coherence_values = []
    model_list = []

    for num_topics in range(8, 15):  # Try a range of topic numbers
        lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=100)
        model_list.append(lda_model)
        coherence_model = CoherenceModel(model=lda_model, texts=tokenized_text, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherence_model.get_coherence())

    # Select the number of topics with the highest coherence value
    optimal_num_topics = range(8, 15)[coherence_values.index(max(coherence_values))]
    return optimal_num_topics

# Determine the optimal number of topics
optimal_topics = get_optimal_num_topics(df['Text'].tolist())

print(f"Optimal number of topics: {optimal_topics}")
from gensim.models import LdaModel
from gensim.corpora import Dictionary

def train_lda_with_optimal_topics(text_data, num_topics):
    tokenized_text = [simple_preprocess(text) for text in text_data]
    dictionary = Dictionary(tokenized_text)
    corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_text]

    # Train the LDA model with the specified number of topics
    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=100)

    return lda_model, corpus

# Train the LDA model with 6 topics
optimal_lda_model, optimal_corpus = train_lda_with_optimal_topics(df['Text'].tolist(), optimal_topics)

# Function to get the dominant topic for a document
def get_dominant_topic(lda_model, doc):
    topics = lda_model.get_document_topics(doc)
    dominant_topic = max(topics, key=lambda x: x[1])
    return dominant_topic[0]  # Returning the topic ID

# Add the topic as a new column in the DataFrame
df['Topic'] = [get_dominant_topic(optimal_lda_model, doc) for doc in optimal_corpus]
topic_counts = df['Topic'].value_counts()
topic_counts

import networkx as nx
from node2vec import Node2Vec
from torch_geometric.data import Data
import torch
from transformers import BertTokenizer, BertModel
import pandas as pd
from fuzzywuzzy import process

# Assuming df is your DataFrame
# Create a graph object
G = nx.Graph()

# Include additional node attributes
for _, row in df.iterrows():
    title = row['Concepts']
    genre = row['Object']
    relation = row['Predicate']

    G.add_node(title)
    G.add_node(genre)
    G.add_edge(title, genre, relation=relation)

# Use BERT for text embeddings with updated embedding size to match Node2Vec (64 dimensions)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

text_embeddings = {}
for column in ['Refrences', 'def', 'Keywords', 'Synonyms']:
    for node in G.nodes():
        matching_rows = df[df['Concepts'] == node]
        if not matching_rows.empty:
            text = matching_rows[column].values[0]
            inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True)
            outputs = model(**inputs)

            # Update the BERT embedding size to match Node2Vec (64 dimensions)
            embedding = outputs['last_hidden_state'][:, 0, :].squeeze().detach().numpy()  # Change the dimension to (64,)

            if node not in text_embeddings:
                text_embeddings[node] = {}
            text_embeddings[node][column] = embedding.tolist()

# Use Node2Vec for node embeddings
node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)
node2vec_model = node2vec.fit(window=10, min_count=1, batch_words=4)
node_embeddings = {node: node2vec_model.wv[node] for node in G.nodes}

# Combine node embeddings with default embedding for missing nodes
default_embedding = torch.zeros(64)  # Change the dimension to match Node2Vec embeddings

combined_embeddings = {}
for node in G.nodes:
    # Check if the node is present in either 'Object' or 'Concepts' column
    if node in df['Concepts'].values:
        combined_embedding = torch.cat([
            torch.tensor(node_embeddings.get(node, default_embedding)),
            torch.tensor(text_embeddings[node]['Refrences']),
            torch.tensor(text_embeddings[node]['def']),
            torch.tensor(text_embeddings[node]['Keywords']),
            torch.tensor(text_embeddings[node]['Synonyms'])
        ])
        combined_embeddings[node] = combined_embedding
    else:
        #print(f"Node '{node}' not found in either 'Object' or 'Concepts' column.")
        continue

# Map node names to numerical indices
node_indices = {node: idx for idx, node in enumerate(G.nodes)}

# Now create the data object for torch_geometric
embedding_matrix = torch.stack(list(combined_embeddings.values()))
edges = torch.tensor([(node_indices[u], node_indices[v]) for u, v in G.edges]).t().contiguous()
edge_index = torch.cat((edges, edges.flip(0)), dim=1)
data = Data(x=embedding_matrix, edge_index=edge_index)
# Assuming df has a 'Topic' column with node labels
node_labels = df['Topic'].astype('category').cat.codes.values

# Add 'y' key to the data object
data.y = torch.tensor(node_labels, dtype=torch.long)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool
from torch_geometric.data import Data
from sklearn.model_selection import train_test_split
import pandas as pd
from torch.optim.lr_scheduler import StepLR
from torch_geometric.nn import GATConv
import random


# Set random seeds for reproducibility
seed = 42
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
np.random.seed(seed)
random.seed(seed)

# Define the GNN model with dropout
class GNNModel(nn.Module):

    def __init__(self, input_dim, embedding_dim=64, num_heads=4, weight_decay=1e-5):
        super(GNNModel, self).__init__()
        self.conv1 = GATConv(input_dim, embedding_dim, heads=num_heads, dropout=0.25)
        self.conv2 = GATConv(embedding_dim * num_heads, embedding_dim, heads=num_heads, dropout=0.25)

        # Calculate the output size after GNN layers
        gnn_output_size = embedding_dim * num_heads * 2  # Adjust if necessary

        # Modify the linear layer to match the output size after GNN
        self.fc = nn.Linear(gnn_output_size, 1)

        # Add L2 regularization to GNN layers
        self.conv1.weight_decay = weight_decay
        self.conv2.weight_decay = weight_decay

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))

        x_mean = global_mean_pool(x, data.batch)
        x_max = global_max_pool(x, data.batch)
        x = torch.cat([x_mean, x_max], dim=1)

        x = self.fc(x)
        return x
    # def __init__(self, input_dim, embedding_dim=64, weight_decay=1e-5):
    #     super(GNNModel, self).__init__()
    #     self.conv1 = GATConv(input_dim, embedding_dim)
    #     self.dropout1 = nn.Dropout(p=0.25)
    #     self.conv2 = GCNConv(embedding_dim, embedding_dim)
    #     self.dropout2 = nn.Dropout(p=0.2)
    #     self.fc1 = nn.Linear(embedding_dim * 2, 64)  # Additional hidden layer
    #     self.fc2 = nn.Linear(64, 1)  # Output layer

    #     # Add L2 regularization to GNN layers
    #     self.conv1.weight_decay = weight_decay
    #     self.conv2.weight_decay = weight_decay
    #     self.fc1.weight_decay = weight_decay
    #     self.fc2.weight_decay = weight_decay

    # def forward(self, data):
    #     x, edge_index = data.x, data.edge_index

    #     x = self.conv1(x, edge_index)
    #     x = F.elu(x)
    #     x = self.dropout1(x)
    #     x = self.conv2(x, edge_index)
    #     x = F.relu(x)
    #     x = self.dropout2(x)

    #     x_mean = global_mean_pool(x, data.batch)
    #     x_max = global_max_pool(x, data.batch)
    #     x = torch.cat([x_mean, x_max], dim=1)

    #     x = F.relu(self.fc1(x))  # Apply ReLU activation to the additional hidden layer
    #     x = self.fc2(x)  # Output layer
    #     return x

# Convert data to a DataFrame
df_data = pd.DataFrame(data.x.numpy(), columns=[f'feat_{i}' for i in range(data.x.shape[1])])
df_data['y'] = df['Topic'][:data.num_nodes].values  # Use only the labels for the existing nodes
df_data.reset_index(drop=True, inplace=True)

# Split data into train, validation, and test sets
train_data, temp_data = train_test_split(df_data, test_size=0.3, random_state=seed)
validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=seed)

# Get the nodes in the training, validation, and test sets
train_nodes = set(train_data.index)
validation_nodes = set(validation_data.index)
test_nodes = set(test_data.index)

# Filter edge indices based on nodes in the training set
train_edge_index = data.edge_index[:, [i for i in range(data.edge_index.shape[1]) if data.edge_index[0, i] in train_nodes or data.edge_index[1, i] in train_nodes]]

# Filter edge indices based on nodes in the validation set
validation_edge_index = data.edge_index[:, [i for i in range(data.edge_index.shape[1]) if data.edge_index[0, i] in validation_nodes or data.edge_index[1, i] in validation_nodes]]

# Filter edge indices based on nodes in the test set
test_edge_index = data.edge_index[:, [i for i in range(data.edge_index.shape[1]) if data.edge_index[0, i] in test_nodes or data.edge_index[1, i] in test_nodes]]

# Convert train, validation, and test sets back to Data objects
train_data = Data(x=torch.tensor(train_data.iloc[:, :-1].values, dtype=torch.float32),
                  edge_index=train_edge_index,  # Use filtered edge_index for training
                  y=torch.tensor(train_data['y'].values, dtype=torch.float32))
validation_data = Data(x=torch.tensor(validation_data.iloc[:, :-1].values, dtype=torch.float32),
                       edge_index=validation_edge_index,  # Use filtered edge_index for validation
                       y=torch.tensor(validation_data['y'].values, dtype=torch.float32))
test_data = Data(x=torch.tensor(test_data.iloc[:, :-1].values, dtype=torch.float32),
                 edge_index=test_edge_index,  # Use filtered edge_index for testing
                 y=torch.tensor(test_data['y'].values, dtype=torch.float32))

# Initialize the GNN model
model = GNNModel(input_dim=embedding_matrix.size(1), embedding_dim=64, num_heads=4)#, dropout=0.5)
# Update input_dim to match the size of the node embeddings

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)

# Define a learning rate scheduler
scheduler = StepLR(optimizer, step_size=20, gamma=0.5)

# Early stopping parameters
best_val_loss = float('inf')
patience = 5  # Adjusted patience
patience_counter = 1

# Validation interval
validation_interval = 5

# Training loop
num_epochs = 200
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(train_data)
    loss = criterion(outputs, train_data.y)
    loss.backward()
    optimizer.step()
    scheduler.step()

    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')

    # Validation and early stopping
    if epoch % validation_interval == 0:
        model.eval()
        with torch.no_grad():
            val_outputs = model(validation_data)
            val_loss = criterion(val_outputs, validation_data.y)

        print(f'Validation Loss: {val_loss.item()}')

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0  # Reset patience counter
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f'Early stopping at epoch {epoch}')
                break

# Evaluation
model.eval()
with torch.no_grad():
    test_outputs = model(test_data).view(-1)  # Reshape to match the shape of test_data.y
    mse = F.mse_loss(test_outputs, test_data.y)
    rmse = torch.sqrt(mse)
    print(f'Mean Squared Error on Test Set: {rmse}')

# 1. User Request Processing:

from transformers import BertTokenizer, BertModel
import torch
from torch_geometric.data import Data


def process_user_request(user_request_text, tokenizer, model):
    # Tokenize the user's request
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model_bet = BertModel.from_pretrained('bert-base-uncased')

    tokens = tokenizer(user_request_text, return_tensors='pt',max_length=1024, truncation=True)
    input_ids = tokens['input_ids']  # Extract input_ids directly

    # Obtain BERT embeddings for the user's request
    with torch.no_grad():
        outputs = model_bet(**tokens)
        user_request_embedding = outputs['last_hidden_state'][:, 0, :].squeeze().detach().numpy()

    # Prepare the data for the GNN model
    user_request_data = Data(x=torch.tensor(user_request_embedding).view(1, -1))  # Adjust dimensions for GNN model
    return user_request_embedding

import torch.nn.functional as F
import torch.nn as nn
import torch

def get_user_request_embedding(user_embedding, embedding_dim, num_heads):
    # Assuming the GNN model has an output dimension of embedding_dim * num_heads * 2
    linear_layer = nn.Linear(embedding_dim * num_heads , 128)

    # Convert the NumPy array back to a PyTorch tensor
    user_embedding_tensor = torch.tensor(user_embedding, dtype=torch.float32)

    user_embedding = F.relu(linear_layer(user_embedding_tensor))

    # Convert the PyTorch tensor to a NumPy array
    user_embedding = user_embedding.detach().numpy()

    return user_embedding

# Assuming you have values for embedding_dim and num_heads
embedding_dim = 768  # replace ... with the actual value
num_heads = 4  # replace ... with the actual value

# 3. Neighborhood Calculation:

def identify_neighbors(user_request_data, graph):
    # Extract the node names from the graph
    nodes = list(graph.nodes)
    print(nodes)

    # Identify the neighboring nodes of the user's request
    # This can be achieved by considering nodes connected to the user's request node
    # For example, using networkx:
    user_request_node = nodes[-1]  # Assuming the user request node is the last node in the graph
    neighbors = list(graph.neighbors(user_request_node))
    return neighbors

def prepare_data_for_GNN(node, user_embedding):
    # Create a minimal Data object with 'x' attribute
    neighbor_data = Data(x=torch.tensor(user_embedding, dtype=torch.float32))

    return neighbor_data

# 4. Recommendation:

# Assuming 'graph' is a PyTorch Geometric Data object
def predict_scores(neighbors, model, user_embedding):
    # For each neighboring node, obtain the GNN model's prediction score
    scores = []
    for neighbor_node in neighbors:
        # Prepare data for GNN model (similar to user_request_data)
        neighbor_data = prepare_data_for_GNN(neighbor_node, user_embedding)

        # Obtain prediction score using the GNN model
        score = model(neighbor_data).item()
        scores.append((neighbor_node, score))

    return scores

# 5. Rank and Top-k Recommendations:

def rank_and_recommend(prediction_scores, k=top_k):
    # Rank the recommended items based on prediction scores
    ranked_items = sorted(prediction_scores, key=lambda x: x[1], reverse=True)

    # Present the top-k recommended items to the user
    top_k_recommendations = ranked_items[:k]
    return top_k_recommendations

































































































user_input_text = "Risk management"


def generate_response(user_input):
    user_input_text = user_input

    # Assuming 'df' is your DataFrame containing the text data
    tokenized_text = [simple_preprocess(text) for text in df['Text'].tolist()]
    dictionary = Dictionary(tokenized_text)

    def get_dominant_topic(lda_model, dictionary, text):
        tokens = simple_preprocess(text)
        bow = dictionary.doc2bow(tokens)
        topics = lda_model.get_document_topics(bow)
        dominant_topic = max(topics, key=lambda x: x[1])
        return dominant_topic[0]  # Returning the topic ID

    # Use the trained LDA model and dictionary
    user_topic = get_dominant_topic(optimal_lda_model, dictionary, user_input_text)

    # Step 3: Filter the DataFrame to get the top five rows with the same topic
    top_five_rows = df[df.apply(lambda row: get_dominant_topic(optimal_lda_model, dictionary, row['Text']) == user_topic, axis=1)].head(top_k)
    # Step 4: Display the information for each row
    output = ''
    for index, row in top_five_rows.iterrows():
        output += f"*{row['Concepts']}"+'\n'
        output += f"*{row['def']}"+'\n'
        output += f"*{row['Keywords']}"+'\n'
        output += f"*{row['Synonyms']}"+'\n'
        output += f"*{row['Refrences']}"+'\n'
        
    return output

    